{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBPdjB8mR94X",
    "outputId": "33a9881b-435b-4452-9586-537267b64990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.0.215-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting matplotlib>=3.3.0 (from ultralytics)\n",
      "  Using cached matplotlib-3.8.2-cp39-cp39-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting numpy>=1.22.2 (from ultralytics)\n",
      "  Using cached numpy-1.26.2-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Using cached scipy-1.11.4-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (2.1.1)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.16.1-cp39-cp39-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Using cached pandas-2.1.3-cp39-cp39-win_amd64.whl.metadata (18 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\shrir\\appdata\\roaming\\python\\python39\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached contourpy-1.2.0-cp39-cp39-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached fonttools-4.45.0-cp39-cp39-win_amd64.whl.metadata (158 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached kiwisolver-1.4.5-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.7.4.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrir\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Using cached ultralytics-8.0.215-py3-none-any.whl (645 kB)\n",
      "Using cached matplotlib-3.8.2-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Using cached numpy-1.26.2-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Using cached pandas-2.1.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Using cached scipy-1.11.4-cp39-cp39-win_amd64.whl (44.3 MB)\n",
      "Using cached seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "Using cached torchvision-0.16.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Using cached contourpy-1.2.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.45.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.5-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Installing collected packages: numpy, kiwisolver, fonttools, cycler, scipy, pandas, contourpy, torchvision, thop, matplotlib, seaborn, ultralytics\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.45.0 kiwisolver-1.4.5 matplotlib-3.8.2 numpy-1.26.2 pandas-2.1.3 scipy-1.11.4 seaborn-0.13.0 thop-0.1.1.post2209072238 torchvision-0.16.1 ultralytics-8.0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.26.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (10.1.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from opencv-python) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip install Pillow\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iPFiXcucpWfz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\shrir\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.version.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_qnCw7UTE6I",
    "outputId": "f077c698-3c19-4e80-a73d-d86fb35d14c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è 'source' is missing. Using 'source=C:\\Users\\shrir\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\ultralytics\\assets'.\n",
      "\n",
      "image 1/2 C:\\Users\\shrir\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x480 3 persons, 1 bus, 8.2ms\n",
      "image 2/2 C:\\Users\\shrir\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\ultralytics\\assets\\zidane.jpg: 384x640 2 persons, 1 tie, 16.9ms\n",
      "Speed: 0.8ms preprocess, 12.5ms inference, 60.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict9\u001b[0m\n",
      "Ultralytics YOLOv8.0.230 üöÄ Python-3.9.18 torch-2.1.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov5nu.pt, data=coco128.yaml, epochs=1, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1760  ultralytics.nn.modules.conv.Conv             [3, 16, 6, 2, 2]              \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      4800  ultralytics.nn.modules.block.C3              [32, 32, 1]                   \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     29184  ultralytics.nn.modules.block.C3              [64, 64, 2]                   \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1]                 \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 14                  -1  1      8320  ultralytics.nn.modules.conv.Conv             [128, 64, 1, 1]               \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     22912  ultralytics.nn.modules.block.C3              [128, 64, 1, False]           \n",
      " 18                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1     74496  ultralytics.nn.modules.block.C3              [128, 128, 1, False]          \n",
      " 21                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 24        [17, 20, 23]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv5n summary: 262 layers, 2654816 parameters, 2654800 gradients, 7.8 GFLOPs\n",
      "\n",
      "Transferred 82/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:02<00:00, 2.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\MACHINE LEARNING\\TL PICA POOL\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\MACHINE LEARNING\\TL PICA POOL\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train6\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      2.45G      2.972      4.679      2.598        178        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.67\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.099    0.00111   0.000107   3.03e-05\n",
      "\n",
      "1 epochs completed in 0.005 hours.\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\last.pt, 5.6MB\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\best.pt, 5.6MB\n",
      "\n",
      "Validating runs\\detect\\train6\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.230 üöÄ Python-3.9.18 torch-2.1.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLOv5n summary (fused): 193 layers, 2649200 parameters, 0 gradients, 7.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.099    0.00111   0.000107   3.06e-05\n",
      "                person        128        254     0.0282     0.0787    0.00757    0.00217\n",
      "               bicycle        128          6          0          0          0          0\n",
      "                   car        128         46          1          0          0          0\n",
      "            motorcycle        128          5          0          0          0          0\n",
      "              airplane        128          6          0          0          0          0\n",
      "                   bus        128          7          0          0          0          0\n",
      "                 train        128          3          0          0          0          0\n",
      "                 truck        128         12          0          0          0          0\n",
      "                  boat        128          6          0          0          0          0\n",
      "         traffic light        128         14          0          0          0          0\n",
      "             stop sign        128          2          0          0          0          0\n",
      "                 bench        128          9          0          0          0          0\n",
      "                  bird        128         16          0          0          0          0\n",
      "                   cat        128          4          0          0          0          0\n",
      "                   dog        128          9          0          0          0          0\n",
      "                 horse        128          2          1          0          0          0\n",
      "              elephant        128         17          0          0          0          0\n",
      "                  bear        128          1          0          0          0          0\n",
      "                 zebra        128          4          0          0          0          0\n",
      "               giraffe        128          9          0          0          0          0\n",
      "              backpack        128          6          0          0          0          0\n",
      "              umbrella        128         18          0          0          0          0\n",
      "               handbag        128         19          0          0          0          0\n",
      "                   tie        128          7          0          0          0          0\n",
      "              suitcase        128          4          0          0          0          0\n",
      "               frisbee        128          5          0          0          0          0\n",
      "                  skis        128          1          0          0          0          0\n",
      "             snowboard        128          7          0          0          0          0\n",
      "           sports ball        128          6          0          0          0          0\n",
      "                  kite        128         10          0          0          0          0\n",
      "          baseball bat        128          4          0          0          0          0\n",
      "        baseball glove        128          7          0          0          0          0\n",
      "            skateboard        128          5          0          0          0          0\n",
      "         tennis racket        128          7          0          0          0          0\n",
      "                bottle        128         18          0          0          0          0\n",
      "            wine glass        128         16          0          0          0          0\n",
      "                   cup        128         36          1          0          0          0\n",
      "                  fork        128          6          0          0          0          0\n",
      "                 knife        128         16          0          0          0          0\n",
      "                 spoon        128         22          0          0          0          0\n",
      "                  bowl        128         28          1          0          0          0\n",
      "                banana        128          1          0          0          0          0\n",
      "              sandwich        128          2          0          0          0          0\n",
      "                orange        128          4          0          0          0          0\n",
      "              broccoli        128         11          0          0          0          0\n",
      "                carrot        128         24          0          0          0          0\n",
      "               hot dog        128          2          0          0          0          0\n",
      "                 pizza        128          5          0          0          0          0\n",
      "                 donut        128         14          0          0          0          0\n",
      "                  cake        128          4          0          0          0          0\n",
      "                 chair        128         35          1          0          0          0\n",
      "                 couch        128          6          0          0          0          0\n",
      "          potted plant        128         14          1          0          0          0\n",
      "                   bed        128          3          0          0          0          0\n",
      "          dining table        128         13          1          0          0          0\n",
      "                toilet        128          2          0          0          0          0\n",
      "                    tv        128          2          0          0          0          0\n",
      "                laptop        128          3          0          0          0          0\n",
      "                 mouse        128          2          0          0          0          0\n",
      "                remote        128          8          0          0          0          0\n",
      "            cell phone        128          8          0          0          0          0\n",
      "             microwave        128          3          0          0          0          0\n",
      "                  oven        128          5          0          0          0          0\n",
      "                  sink        128          6          0          0          0          0\n",
      "          refrigerator        128          5          0          0          0          0\n",
      "                  book        128         29          0          0          0          0\n",
      "                 clock        128          9          0          0          0          0\n",
      "                  vase        128          2          0          0          0          0\n",
      "              scissors        128          1          0          0          0          0\n",
      "            teddy bear        128         21          0          0          0          0\n",
      "            toothbrush        128          5          0          0          0          0\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train6\u001b[0m\n",
      "Ultralytics YOLOv8.0.230 üöÄ Python-3.9.18 torch-2.1.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLOv5n summary (fused): 193 layers, 2649200 parameters, 0 gradients, 7.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\MACHINE LEARNING\\TL PICA POOL\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 \u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.099    0.00128   0.000122   3.38e-05\n",
      "                person        128        254     0.0297     0.0906    0.00865     0.0024\n",
      "               bicycle        128          6          0          0          0          0\n",
      "                   car        128         46          1          0          0          0\n",
      "            motorcycle        128          5          0          0          0          0\n",
      "              airplane        128          6          0          0          0          0\n",
      "                   bus        128          7          0          0          0          0\n",
      "                 train        128          3          0          0          0          0\n",
      "                 truck        128         12          0          0          0          0\n",
      "                  boat        128          6          0          0          0          0\n",
      "         traffic light        128         14          0          0          0          0\n",
      "             stop sign        128          2          0          0          0          0\n",
      "                 bench        128          9          0          0          0          0\n",
      "                  bird        128         16          0          0          0          0\n",
      "                   cat        128          4          0          0          0          0\n",
      "                   dog        128          9          0          0          0          0\n",
      "                 horse        128          2          1          0          0          0\n",
      "              elephant        128         17          0          0          0          0\n",
      "                  bear        128          1          0          0          0          0\n",
      "                 zebra        128          4          0          0          0          0\n",
      "               giraffe        128          9          0          0          0          0\n",
      "              backpack        128          6          0          0          0          0\n",
      "              umbrella        128         18          0          0          0          0\n",
      "               handbag        128         19          0          0          0          0\n",
      "                   tie        128          7          0          0          0          0\n",
      "              suitcase        128          4          0          0          0          0\n",
      "               frisbee        128          5          0          0          0          0\n",
      "                  skis        128          1          0          0          0          0\n",
      "             snowboard        128          7          0          0          0          0\n",
      "           sports ball        128          6          0          0          0          0\n",
      "                  kite        128         10          0          0          0          0\n",
      "          baseball bat        128          4          0          0          0          0\n",
      "        baseball glove        128          7          0          0          0          0\n",
      "            skateboard        128          5          0          0          0          0\n",
      "         tennis racket        128          7          0          0          0          0\n",
      "                bottle        128         18          0          0          0          0\n",
      "            wine glass        128         16          0          0          0          0\n",
      "                   cup        128         36          1          0          0          0\n",
      "                  fork        128          6          0          0          0          0\n",
      "                 knife        128         16          0          0          0          0\n",
      "                 spoon        128         22          0          0          0          0\n",
      "                  bowl        128         28          1          0          0          0\n",
      "                banana        128          1          0          0          0          0\n",
      "              sandwich        128          2          0          0          0          0\n",
      "                orange        128          4          0          0          0          0\n",
      "              broccoli        128         11          0          0          0          0\n",
      "                carrot        128         24          0          0          0          0\n",
      "               hot dog        128          2          0          0          0          0\n",
      "                 pizza        128          5          0          0          0          0\n",
      "                 donut        128         14          0          0          0          0\n",
      "                  cake        128          4          0          0          0          0\n",
      "                 chair        128         35          1          0          0          0\n",
      "                 couch        128          6          0          0          0          0\n",
      "          potted plant        128         14          1          0          0          0\n",
      "                   bed        128          3          0          0          0          0\n",
      "          dining table        128         13          1          0          0          0\n",
      "                toilet        128          2          0          0          0          0\n",
      "                    tv        128          2          0          0          0          0\n",
      "                laptop        128          3          0          0          0          0\n",
      "                 mouse        128          2          0          0          0          0\n",
      "                remote        128          8          0          0          0          0\n",
      "            cell phone        128          8          0          0          0          0\n",
      "             microwave        128          3          0          0          0          0\n",
      "                  oven        128          5          0          0          0          0\n",
      "                  sink        128          6          0          0          0          0\n",
      "          refrigerator        128          5          0          0          0          0\n",
      "                  book        128         29          0          0          0          0\n",
      "                 clock        128          9          0          0          0          0\n",
      "                  vase        128          2          0          0          0          0\n",
      "              scissors        128          1          0          0          0          0\n",
      "            teddy bear        128         21          0          0          0          0\n",
      "            toothbrush        128          5          0          0          0          0\n",
      "Speed: 0.9ms preprocess, 13.5ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train62\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#YOLO v5 model\n",
    "#Importing YOLO with saved weights:\n",
    "\n",
    "#model = YOLO('FAKE POOL DETECTION/YOLO WEIGHTS/best.pt')\n",
    "#results = model(save=True,conf=0.4)\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "\n",
    "model = YOLO('yolov5nu.pt')\n",
    "results = model(save=True,conf=0.4)\n",
    "results = model.train(data='coco128.yaml', epochs=1)         # -> Training on COCO128 DATASET\n",
    "results = model.val()\n",
    "\n",
    "# After training best model weights are stored in runs/detect/train/weights/best.pt\n",
    "# we can use these weights directly next time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHOJuFvZX6uN",
    "outputId": "6dc8a64c-5309-47b0-f901-84e32ca42618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "3HCXkvxZYOFc",
    "outputId": "ad738311-7d1e-4747-81e6-8812f46b90fd"
   },
   "outputs": [],
   "source": [
    "# %mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzXtrcdbYSmo"
   },
   "outputs": [],
   "source": [
    "#!tar -xvf \"/content/drive/MyDrive/ML projekts/TL_ML/PASCAL VOC DATASET/VOCtrainval_06-Nov-2007.tar\" -C \"/content/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AD6dyOYSTpxS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000005.jpg: 480x640 4 chairs, 1 clock, 203.4ms\n",
      "Speed: 649.5ms preprocess, 203.4ms inference, 10.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train63\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000007.jpg: 448x640 1 car, 194.3ms\n",
      "Speed: 0.0ms preprocess, 194.3ms inference, 16.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train64\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000009.jpg: 480x640 3 persons, 1 horse, 48.7ms\n",
      "Speed: 7.8ms preprocess, 48.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train65\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000012.jpg: 448x640 1 truck, 41.2ms\n",
      "Speed: 10.4ms preprocess, 41.2ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train66\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000016.jpg: 640x448 (no detections), 174.9ms\n",
      "Speed: 12.0ms preprocess, 174.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train67\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000017.jpg: 512x640 1 person, 2 horses, 153.6ms\n",
      "Speed: 8.7ms preprocess, 153.6ms inference, 5.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train68\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000019.jpg: 480x640 2 cats, 44.8ms\n",
      "Speed: 8.3ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train69\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000020.jpg: 640x480 (no detections), 75.3ms\n",
      "Speed: 8.6ms preprocess, 75.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train610\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000021.jpg: 640x448 3 persons, 36.9ms\n",
      "Speed: 8.1ms preprocess, 36.9ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train611\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000023.jpg: 640x448 2 persons, 1 bicycle, 40.3ms\n",
      "Speed: 8.2ms preprocess, 40.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train612\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000024.jpg: 448x640 (no detections), 39.4ms\n",
      "Speed: 10.4ms preprocess, 39.4ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train613\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000026.jpg: 448x640 1 car, 20.3ms\n",
      "Speed: 5.5ms preprocess, 20.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train614\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000030.jpg: 480x640 2 persons, 1 bicycle, 24.6ms\n",
      "Speed: 0.0ms preprocess, 24.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train615\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000032.jpg: 384x640 2 persons, 1 airplane, 144.5ms\n",
      "Speed: 6.5ms preprocess, 144.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train616\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000033.jpg: 480x640 1 airplane, 23.1ms\n",
      "Speed: 5.9ms preprocess, 23.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train617\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000034.jpg: 640x480 1 train, 24.3ms\n",
      "Speed: 6.3ms preprocess, 24.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train618\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000035.jpg: 480x640 2 persons, 7 cups, 24.4ms\n",
      "Speed: 0.0ms preprocess, 24.4ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train619\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000036.jpg: 640x448 1 dog, 29.2ms\n",
      "Speed: 0.0ms preprocess, 29.2ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train620\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000039.jpg: 480x640 1 tv, 1 mouse, 1 keyboard, 24.7ms\n",
      "Speed: 8.5ms preprocess, 24.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train621\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000041.jpg: 448x640 2 persons, 1 tv, 28.2ms\n",
      "Speed: 0.0ms preprocess, 28.2ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train622\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000042.jpg: 448x640 3 trains, 16.6ms\n",
      "Speed: 8.1ms preprocess, 16.6ms inference, 8.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train623\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000044.jpg: 448x640 1 cat, 18.7ms\n",
      "Speed: 9.1ms preprocess, 18.7ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train624\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000046.jpg: 640x448 1 bird, 17.6ms\n",
      "Speed: 3.8ms preprocess, 17.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train625\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000047.jpg: 448x640 1 truck, 28.6ms\n",
      "Speed: 8.1ms preprocess, 28.6ms inference, 3.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train626\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000048.jpg: 640x480 (no detections), 23.5ms\n",
      "Speed: 8.0ms preprocess, 23.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train627\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000050.jpg: 480x640 5 persons, 4 bottles, 1 cup, 3 bowls, 26.5ms\n",
      "Speed: 0.0ms preprocess, 26.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train628\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000051.jpg: 480x640 2 motorcycles, 33.2ms\n",
      "Speed: 0.0ms preprocess, 33.2ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train629\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000052.jpg: 448x640 1 potted plant, 21.6ms\n",
      "Speed: 0.0ms preprocess, 21.6ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train630\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000060.jpg: 448x640 2 cars, 23.9ms\n",
      "Speed: 9.2ms preprocess, 23.9ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train631\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000061.jpg: 448x640 3 boats, 18.2ms\n",
      "Speed: 10.0ms preprocess, 18.2ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train632\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000063.jpg: 480x640 1 dog, 1 couch, 1 keyboard, 16.6ms\n",
      "Speed: 2.2ms preprocess, 16.6ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train633\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000064.jpg: 640x640 1 dog, 30.5ms\n",
      "Speed: 10.8ms preprocess, 30.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train634\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000065.jpg: 448x640 1 dog, 24.1ms\n",
      "Speed: 4.2ms preprocess, 24.1ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train635\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000066.jpg: 480x640 3 persons, 26.4ms\n",
      "Speed: 0.0ms preprocess, 26.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train636\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000072.jpg: 640x448 1 motorcycle, 28.5ms\n",
      "Speed: 0.0ms preprocess, 28.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train637\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000073.jpg: 640x480 1 person, 13.3ms\n",
      "Speed: 10.9ms preprocess, 13.3ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train638\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000077.jpg: 448x640 1 cat, 19.5ms\n",
      "Speed: 8.0ms preprocess, 19.5ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train639\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000078.jpg: 544x640 2 dogs, 1 couch, 159.6ms\n",
      "Speed: 7.8ms preprocess, 159.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train640\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000081.jpg: 448x640 5 persons, 18.3ms\n",
      "Speed: 0.0ms preprocess, 18.3ms inference, 6.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train641\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000083.jpg: 448x640 7 persons, 22.1ms\n",
      "Speed: 10.1ms preprocess, 22.1ms inference, 8.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train642\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000089.jpg: 480x640 3 persons, 1 tv, 25.6ms\n",
      "Speed: 6.0ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train643\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000091.jpg: 480x640 1 person, 7 cars, 24.6ms\n",
      "Speed: 0.0ms preprocess, 24.6ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train644\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000093.jpg: 448x640 1 dog, 1 couch, 26.8ms\n",
      "Speed: 4.5ms preprocess, 26.8ms inference, 8.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train645\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000095.jpg: 416x640 1 train, 159.7ms\n",
      "Speed: 2.1ms preprocess, 159.7ms inference, 10.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train646\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000099.jpg: 480x640 1 cat, 1 chair, 30.5ms\n",
      "Speed: 4.7ms preprocess, 30.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train647\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000101.jpg: 448x640 1 person, 1 chair, 15.7ms\n",
      "Speed: 8.0ms preprocess, 15.7ms inference, 6.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train648\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000102.jpg: 480x640 2 couchs, 1 remote, 17.1ms\n",
      "Speed: 4.1ms preprocess, 17.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train649\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000104.jpg: 448x640 2 persons, 1 tv, 18.1ms\n",
      "Speed: 8.0ms preprocess, 18.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train650\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000107.jpg: 480x640 1 horse, 4 sheeps, 1 cow, 21.7ms\n",
      "Speed: 1.1ms preprocess, 21.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train651\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000109.jpg: 352x640 (no detections), 175.3ms\n",
      "Speed: 0.0ms preprocess, 175.3ms inference, 0.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train652\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000110.jpg: 480x640 3 persons, 19.9ms\n",
      "Speed: 5.0ms preprocess, 19.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train653\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000112.jpg: 640x448 (no detections), 23.2ms\n",
      "Speed: 0.0ms preprocess, 23.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train654\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000113.jpg: 448x640 1 person, 22.5ms\n",
      "Speed: 0.0ms preprocess, 22.5ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train655\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000117.jpg: 448x640 1 airplane, 16.5ms\n",
      "Speed: 8.4ms preprocess, 16.5ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train656\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000118.jpg: 480x640 1 bird, 1 cat, 22.4ms\n",
      "Speed: 0.0ms preprocess, 22.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train657\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000120.jpg: 512x640 1 dog, 29.9ms\n",
      "Speed: 4.3ms preprocess, 29.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train658\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000121.jpg: 480x640 2 tvs, 1 keyboard, 18.3ms\n",
      "Speed: 3.6ms preprocess, 18.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train659\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000122.jpg: 640x640 1 cat, 26.8ms\n",
      "Speed: 7.0ms preprocess, 26.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train660\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000123.jpg: 480x640 1 train, 25.2ms\n",
      "Speed: 0.0ms preprocess, 25.2ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train661\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000125.jpg: 480x640 3 persons, 1 bicycle, 15.9ms\n",
      "Speed: 7.2ms preprocess, 15.9ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train662\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000129.jpg: 640x448 3 persons, 18.4ms\n",
      "Speed: 0.0ms preprocess, 18.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train663\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000130.jpg: 480x640 (no detections), 26.1ms\n",
      "Speed: 0.0ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train664\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000131.jpg: 448x640 1 car, 26.0ms\n",
      "Speed: 4.1ms preprocess, 26.0ms inference, 7.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train665\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000132.jpg: 544x640 2 persons, 1 bus, 23.9ms\n",
      "Speed: 8.3ms preprocess, 23.9ms inference, 10.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train666\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000133.jpg: 640x576 1 person, 2 horses, 191.2ms\n",
      "Speed: 7.1ms preprocess, 191.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Results saved to \u001b[1mruns\\detect\\train667\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000134.jpg: 480x640 1 car, 19.2ms\n",
      "Speed: 5.0ms preprocess, 19.2ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train668\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000138.jpg: 448x640 4 persons, 2 cups, 1 couch, 19.7ms\n",
      "Speed: 6.5ms preprocess, 19.7ms inference, 9.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train669\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000140.jpg: 544x640 1 dog, 1 couch, 24.9ms\n",
      "Speed: 3.7ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train670\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000141.jpg: 544x640 1 bird, 18.9ms\n",
      "Speed: 11.6ms preprocess, 18.9ms inference, 8.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train671\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000142.jpg: 480x640 1 car, 16.3ms\n",
      "Speed: 2.6ms preprocess, 16.3ms inference, 10.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train672\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000143.jpg: 448x640 1 cat, 1 dog, 21.5ms\n",
      "Speed: 8.3ms preprocess, 21.5ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train673\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000146.jpg: 640x480 1 person, 22.7ms\n",
      "Speed: 0.0ms preprocess, 22.7ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train674\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000147.jpg: 448x640 (no detections), 18.7ms\n",
      "Speed: 10.1ms preprocess, 18.7ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train675\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000150.jpg: 448x640 4 persons, 1 horse, 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 8.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train676\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000153.jpg: 480x640 2 cars, 27.3ms\n",
      "Speed: 0.0ms preprocess, 27.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train677\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000154.jpg: 448x640 2 boats, 26.0ms\n",
      "Speed: 0.0ms preprocess, 26.0ms inference, 18.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train678\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000156.jpg: 480x640 1 car, 1 bench, 18.1ms\n",
      "Speed: 7.6ms preprocess, 18.1ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train679\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000158.jpg: 480x640 1 bird, 1 frisbee, 18.5ms\n",
      "Speed: 7.5ms preprocess, 18.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train680\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000159.jpg: 448x640 1 truck, 16.1ms\n",
      "Speed: 4.2ms preprocess, 16.1ms inference, 6.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train681\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000161.jpg: 512x640 1 car, 1 truck, 19.5ms\n",
      "Speed: 1.1ms preprocess, 19.5ms inference, 7.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train682\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000162.jpg: 480x640 1 person, 1 tv, 24.5ms\n",
      "Speed: 0.0ms preprocess, 24.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train683\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000163.jpg: 576x640 1 person, 1 motorcycle, 247.3ms\n",
      "Speed: 8.2ms preprocess, 247.3ms inference, 0.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train684\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000164.jpg: 480x640 1 person, 1 motorcycle, 27.4ms\n",
      "Speed: 8.3ms preprocess, 27.4ms inference, 15.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train685\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000165.jpg: 512x640 1 bed, 25.0ms\n",
      "Speed: 3.9ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train686\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000169.jpg: 448x640 4 persons, 1 car, 1 train, 17.9ms\n",
      "Speed: 6.1ms preprocess, 17.9ms inference, 6.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train687\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000170.jpg: 480x640 2 persons, 4 bottles, 1 cup, 26.1ms\n",
      "Speed: 0.0ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train688\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000171.jpg: 640x480 1 person, 1 dog, 12.4ms\n",
      "Speed: 8.0ms preprocess, 12.4ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\train689\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000173.jpg: 640x544 1 person, 2 horses, 234.8ms\n",
      "Speed: 6.3ms preprocess, 234.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Results saved to \u001b[1mruns\\detect\\train690\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000174.jpg: 448x640 1 person, 1 chair, 24.0ms\n",
      "Speed: 8.4ms preprocess, 24.0ms inference, 10.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train691\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000177.jpg: 480x640 7 persons, 3 cups, 4 bowls, 1 dining table, 19.2ms\n",
      "Speed: 6.0ms preprocess, 19.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train692\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000180.jpg: 480x640 1 car, 1 truck, 20.8ms\n",
      "Speed: 5.5ms preprocess, 20.8ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train693\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000184.jpg: 480x640 2 boats, 19.8ms\n",
      "Speed: 3.7ms preprocess, 19.8ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train694\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000187.jpg: 480x640 1 tv, 20.7ms\n",
      "Speed: 6.0ms preprocess, 20.7ms inference, 10.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train695\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000189.jpg: 480x640 1 bird, 15.6ms\n",
      "Speed: 5.6ms preprocess, 15.6ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train696\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000190.jpg: 480x640 2 persons, 4 buss, 17.3ms\n",
      "Speed: 8.0ms preprocess, 17.3ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train697\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000192.jpg: 480x640 1 person, 1 chair, 1 dining table, 22.7ms\n",
      "Speed: 0.0ms preprocess, 22.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train698\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000193.jpg: 480x640 4 persons, 1 couch, 1 book, 24.4ms\n",
      "Speed: 4.9ms preprocess, 24.4ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train699\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000194.jpg: 448x640 2 horses, 19.4ms\n",
      "Speed: 7.0ms preprocess, 19.4ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6100\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000198.jpg: 448x640 1 train, 18.3ms\n",
      "Speed: 4.8ms preprocess, 18.3ms inference, 6.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6101\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000200.jpg: 320x640 6 persons, 1 bottle, 250.3ms\n",
      "Speed: 4.6ms preprocess, 250.3ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6102\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000203.jpg: 448x640 1 chair, 1 bed, 25.7ms\n",
      "Speed: 0.0ms preprocess, 25.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6103\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000207.jpg: 640x448 1 bus, 17.7ms\n",
      "Speed: 6.8ms preprocess, 17.7ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\train6104\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000208.jpg: 480x640 2 chairs, 1 couch, 1 tv, 23.4ms\n",
      "Speed: 0.0ms preprocess, 23.4ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6105\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000209.jpg: 480x640 1 cat, 1 bed, 14.3ms\n",
      "Speed: 2.1ms preprocess, 14.3ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6106\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000210.jpg: 480x640 3 persons, 18.8ms\n",
      "Speed: 8.3ms preprocess, 18.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6107\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000211.jpg: 480x640 2 cows, 16.6ms\n",
      "Speed: 7.1ms preprocess, 16.6ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6108\u001b[0m\n",
      "\n",
      "image 1/1 C:\\MACHINE LEARNING\\TL PICA POOL\\FAKE POOL DETECTION\\VOCdevkit_train\\VOC2007\\JPEGImages\\000214.jpg: 480x640 1 horse, 27.1ms\n",
      "Speed: 8.0ms preprocess, 27.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\train6109\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[0;32m      3\u001b[0m   new_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,i)\n\u001b[1;32m----> 4\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\engine\\model.py:98\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the predict() method with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\engine\\model.py:257\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\engine\\predictor.py:265\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 265\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\engine\\predictor.py:137\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[0;32m    136\u001b[0m                            mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:356\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    353\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\tasks.py:60\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\tasks.py:81\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m     82\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:59\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     box, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m x_cat\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m dbox \u001b[38;5;241m=\u001b[39m dist2bbox(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), xywh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medgetpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     img_h \u001b[38;5;241m=\u001b[39m shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:33\u001b[0m, in \u001b[0;36mDFL.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a transformer layer on input tensor 'x' and returns a tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m b, c, a \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# batch, channels, anchors\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m4\u001b[39m, a)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path='FAKE POOL DETECTION/VOCdevkit_train/VOC2007/JPEGImages'\n",
    "for i in os.listdir(path):\n",
    "  new_path=os.path.join(path,i)\n",
    "  results = model(new_path,save=True, conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "050xSkRmqmfM"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_8OqxMb7aH9Z"
   },
   "outputs": [],
   "source": [
    "n=50\n",
    "for i in range(n):\n",
    "      try:\n",
    "        path_2='/content/runs/detect/train' +str(i+3)\n",
    "        img_path=os.listdir(path_2)\n",
    "        print(img_path[0])\n",
    "        img=cv2.imread(img_path[0])\n",
    "        cv2.imshow(img)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XRnHgsYoz6v"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('/content/runs/detect/train12/005388.jpg')\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHrRuQFx48t3"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('/content/runs/detect/train37/003466.jpg')\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW-IslTE5K4d"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('/content/runs/detect/train50/000044.jpg')\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbMqAdHl5YKa"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('/content/runs/detect/train16/001010.jpg')\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8WTZgV35lIO"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('/content/runs/detect/train27/006632.jpg')\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQgeUgM_5sNi"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('/content/drive/MyDrive/ML projekts/TL_ML/images.zip','r') as zipobj:\n",
    "  zipobj.extractall('/content/TL_TASK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SX8NKw_j6hxn"
   },
   "outputs": [],
   "source": [
    "path='/content/TL_TASK/images'\n",
    "for i in os.listdir(path):\n",
    "  new_path=os.path.join(path,i)\n",
    "  results = model(new_path,save=True, conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjSb313v7Hcu"
   },
   "outputs": [],
   "source": [
    "results = model('/content/TL_TASK/images/image134.jpg', conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac0lblmA7VXn"
   },
   "outputs": [],
   "source": [
    "results = model('/content/TL_TASK/images/image169.jpg', conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-ClBviU8pSn"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If NO DETECTIONS OR ( DETECTIONS STRICTLY RELATED TO EVERYDAY PRODUCTS + TEXT EXTRACTION POSITIVE):\n",
    "  PROCEED WITH ITT PROMPT\n",
    "ELSE IF DETECTIONS FROM 79 CLASSES WITH HIGH CONF SCORE:\n",
    "  DISCARD IT\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
